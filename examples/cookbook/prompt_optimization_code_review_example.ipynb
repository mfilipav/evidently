{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a7ac63",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“Š Prompt Optimization with Evidently: Code Review Quality Classifier\n",
    "\n",
    "This tutorial demonstrates how to use Evidently's new `PromptOptimizer` API for optimizing prompts for LLM judges. \n",
    "We'll walk through optimizing a prompt that classifies the quality of code reviews written for junior developers.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… What you'll learn:\n",
    "- How to set up a dataset for LLM evaluation\n",
    "- How to define an LLM judge with a prompt template\n",
    "- How to run the prompt optimization loop\n",
    "- How to retrieve and inspect the best performing prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed the required packages yet:\n",
    "# !pip install evidently openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc9af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evidently import Dataset, DataDefinition, LLMClassification\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
    "from evidently.llm.models import LLMMessage\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.optimization import PromptOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5f6441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated review</th>\n",
       "      <th>Expert label</th>\n",
       "      <th>Expert comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This implementation appears to work, but the a...</td>\n",
       "      <td>bad</td>\n",
       "      <td>The tone is slighly condescending, no actionab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great job! Keep it up!</td>\n",
       "      <td>bad</td>\n",
       "      <td>Not actionable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It would be advisable to think about modularit...</td>\n",
       "      <td>bad</td>\n",
       "      <td>there is a suggestion, but no real guidance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youâ€™ve structured the class very well, and the...</td>\n",
       "      <td>good</td>\n",
       "      <td>Good tone, actionable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great job! This is clean and well-organized. T...</td>\n",
       "      <td>bad</td>\n",
       "      <td>Pure praise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Generated review  ...                                     Expert comment\n",
       "0  This implementation appears to work, but the a...  ...  The tone is slighly condescending, no actionab...\n",
       "1                             Great job! Keep it up!  ...                                     Not actionable\n",
       "2  It would be advisable to think about modularit...  ...        there is a suggestion, but no real guidance\n",
       "3  Youâ€™ve structured the class very well, and the...  ...                              Good tone, actionable\n",
       "4  Great job! This is clean and well-organized. T...  ...                                        Pure praise\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "review_dataset = pd.read_csv(\"../datasets/code_review.csv\")\n",
    "review_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e464810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how Evidently should interpret your dataset\n",
    "dd = DataDefinition(\n",
    "    text_columns=[\"Generated review\", \"Expert comment\"],\n",
    "    categorical_columns=[\"Expert label\"],\n",
    "    llm=LLMClassification(\n",
    "        input=\"Generated review\",\n",
    "        target=\"Expert label\",\n",
    "        reasoning=\"Expert comment\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3957c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your pandas DataFrame into an Evidently Dataset\n",
    "dataset = Dataset.from_pandas(\n",
    "    data=review_dataset,\n",
    "    data_definition=dd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af027bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template and judge for classifying code review quality\n",
    "criteria = \"\"\"A review is GOOD when it's actionable and constructive.\n",
    "A review is BAD when it is non-actionable or overly critical.\"\"\"\n",
    "\n",
    "pre_messages = [\n",
    "    LLMMessage(\n",
    "        role=\"system\",\n",
    "        content=\"\"\"You are evaluating the quality of code reviews given to junior developers.\"\"\"\n",
    "    ),\n",
    "]\n",
    "     \n",
    "feedback_quality = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=pre_messages,\n",
    "    criteria=criteria,\n",
    "    target_category=\"bad\",\n",
    "    non_target_category=\"good\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e03478f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = LLMEval(\n",
    "    alias=\"Code Review Judge\",\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    column_name=\"Generated review\",\n",
    "    template=feedback_quality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6995309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed prompt 'A review is GOOD when it's actionable an...', got preds(50) preds_reasoning(50)\n",
      "Prompt scored: AccuracyScorer: 0.62\n",
      "Prompt 'A review is GOOD when it's actionable an...' optimized to 'A review is GOOD when it provides action...'\n",
      "Executed prompt 'A review is GOOD when it provides action...', got preds(50) preds_reasoning(50)\n",
      "Prompt scored: AccuracyScorer: 0.84\n",
      "Prompt 'A review is GOOD when it provides action...' optimized to 'A review is GOOD when it provides action...'\n",
      "Executed prompt 'A review is GOOD when it provides action...', got preds(50) preds_reasoning(50)\n",
      "Prompt scored: AccuracyScorer: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizer and run optimization using feedback strategy\n",
    "optimizer = PromptOptimizer(\n",
    "    name=\"code_review_example\",\n",
    "    strategy=\"feedback\",\n",
    "    checkpoint_path=None,\n",
    ")\n",
    "optimizer.set_input_dataset(dataset=dataset)\n",
    "await optimizer.arun(\n",
    "    executor=judge,\n",
    "    scorer=\"accuracy\"\n",
    ")\n",
    "# for sync version:\n",
    "# optimizer.run(judge, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7f3162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A review is GOOD when it provides actionable and constructive feedback that encourages improvement.\n",
      "A review is BAD when it is non-actionable, overly critical, or lacks specificity, making it unclear how to improve.\n"
     ]
    }
   ],
   "source": [
    "# Show the best-performing prompt template found by the optimizer\n",
    "print(optimizer.best_prompt())\n",
    "# starting prompt:\n",
    "# criteria = \"\"\"A review is GOOD when it's actionable and constructive.\n",
    "# A review is BAD when it is non-actionable or overly critical.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aef48c-4369-4178-87ca-fd590eb2fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
